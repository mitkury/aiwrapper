import { config } from "process";
import { LangOptions, LanguageProvider } from "../../language-provider.ts";
import { LangContentPart, LangImageInput, LangMessage, LangMessages } from "../../messages.ts";
import { OpenAILikeConfig } from "../openai-chat-completions-lang.ts";
import { OpenAIResponsesOptions } from "../openai-responses-lang.ts";
import { BodyPartForOpenAIResponses, prepareBodyPartForOpenAIResponsesAPI } from "./openai-responses-messages.ts";
import { processResponseStream } from "../../../process-response-stream.ts";

// We're not going to define all the things that go into responses, there a LOT of them;
// Check the autogenerated types from openai here: @todo link
export class InProgressOpenAIResponse {
  id: string;
  items: OpenAIResponseItem[];
  messages: LangMessages;
  onResult?: (result: LangMessage) => void;

  constructor(messages: LangMessages, onResult?: (result: LangMessage) => void) {
    this.items = [];
    this.messages = messages;
    this.onResult = onResult;
  }

  handleEvent(data: any) {
    if (!('type' in data)) {
      console.warn('Unknown data from server:', data);
      return;
    }

    if (!('type' in data)) {
      console.warn('Unknown data from server:', data);
      return;
    }

    switch (data.type) {
      case 'response.created':
        this.id = data.response.id;
        break;
      case 'response.output_item.added':
        this.addItem(data.item);
        break;
      case 'response.output_item.done':
        this.setItem(data.item);
        break;
      case 'response.content_part.added':
        break;
      case 'response.content_part.done':
        break;

      // Deltas that we care about (feel free to add more if you want to show them in-progress somewher)
      case 'response.output_text.delta':
        //case 'response.function_call_arguments.delta':
        this.applyDelta(data.item_id, data.delta);
        break;
    }
  }

  setItem(target: OpenAIResponseItem) {
    //data: {"type":"response.output_item.added","sequence_number":4,"output_index":1,"item":{"id":"fc_0d1fd1ec5aba34630068edbe1df02881a28c623f7dc5d45e81","type":"function_call","status":"in_progress","arguments":"","call_id":"call_Pc4gzmrkhNIdTYrySlALhyIl","name":"get_current_weather"}}
    //data: {"type":"response.output_item.done","sequence_number":18,"output_index":1,"item":{"id":"fc_0d1fd1ec5aba34630068edbe1df02881a28c623f7dc5d45e81","type":"function_call","status":"completed","arguments":"{\"location\":\"Boston, MA\",\"unit\":\"celsius\"}","call_id":"call_Pc4gzmrkhNIdTYrySlALhyIl","name":"get_current_weather"}}

    // data: {"type":"response.output_item.added","sequence_number":4,"output_index":1,"item":{"id":"msg_0dfe4783196ccc2e0068edc3874d5c81a3b2beb308e6eae8f3","type":"message","status":"in_progress","content":[],"role":"assistant"}}
    //data: {"type":"response.output_item.done","sequence_number":236,"output_index":1,"item":{"id":"msg_0dfe4783196ccc2e0068edc3874d5c81a3b2beb308e6eae8f3","type":"message","status":"completed","content":[{"type":"output_text","annotations":[],"logprobs":[],"text":"Hey!"}],"role":"assistant"}}

    const item = this.getItem(target.id);
    if (!target) {
      console.warn('Unknown item:', target);
      return;
    }

    if (!item.targetMessage) {
      console.warn('Unknown target message for item:', target);
      return;
    }

    switch (item.type) {
      case 'message':
        if (item.role === 'assistant') {
          for (const content of target.content) {
            if (content.type === 'output_text') {
              item.targetMessage.content = content.text as string;
            }
          }

          this.onResult?.(item.targetMessage);
        } else {
          console.warn('Unknown role:', item.role, 'for item:', item);
        }
        break;

      case 'function_call':
        //"item":{"id":"fc_0d1fd1ec5aba34630068edbe1df02881a28c623f7dc5d45e81","type":"function_call","status":"completed","arguments":"{\"location\":\"Boston, MA\",\"unit\":\"celsius\"}","call_id":"call_Pc4gzmrkhNIdTYrySlALhyIl","name":"get_current_weather"
        
        const argsParsed = JSON.parse(target.arguments) as Record<string, any>;
        const callId = target.call_id;
        const name = target.name;

        this.messages.addAssistantToolCalls([
          {
            callId,
            name,
            arguments: argsParsed
          }
        ]);
    }
  }

  addItem(target: OpenAIResponseItem) {
    this.items.push(target);

    if (target.type === 'message') {
      if (target.role === 'assistant') {
        this.messages.addAssistantMessage(target.text ?? '');
        target.targetMessage = this.messages[this.messages.length - 1];
        // We set the openaiResponseId so we effectively respond to this message
        // without re-sending all of the previous messages (check how we prepare input for the API when we send messages)
        target.targetMessage.meta = { openaiResponseId: target.id };
        this.onResult?.(target.targetMessage);
      } else {
        console.warn('Unknown role:', target.role, 'for item:', target);
      }
    }
  }

  getItem(id: string): OpenAIResponseItem | undefined {
    return this.items.find(r => r.id === id);
  }

  applyDelta(itemId: string, delta: any) {
    const item = this.getItem(itemId);
    if (item) {
      if (typeof delta === "string") {
        item.text += delta;
      }

      if (item.targetMessage) {
        // @TODO: we need to update different deltas correctly
        if (typeof delta === "string") {
          item.targetMessage.content += delta;
        } else {
          console.warn('Unknown delta type:', typeof delta, 'for item:', item);
        }

        // This callback is reponsible for the real-time vizualizaiton of the model output.
        // So we can show the output being generated in UIs.
        this.onResult?.(item.targetMessage);
      }
    }
  }
}

export type OpenAIResponseItem = {
  id: string;
  type: string;
  // We link our messages to items so we can mutate them as items are updated
  targetMessage?: LangMessage;
  [key: string]: any;
}

export class OpenAIResponsesLangTwo extends LanguageProvider {

  private model: string;
  private apiKey: string;
  private baseURL = "https://api.openai.com/v1";

  constructor(options: OpenAIResponsesOptions) {
    super("OpenAI Responses");

    this.model = options.model;
    this.apiKey = options.apiKey;
  }

  async ask(prompt: string, options?: LangOptions): Promise<LangMessages> {
    const messages = new LangMessages();

    messages.push({ role: "user", content: prompt });

    return this.chat(messages, options);
  }

  async chat(messages: LangMessage[] | LangMessages, options?: LangOptions): Promise<LangMessages> {
    const msgCollection = messages instanceof LangMessages
      ? messages
      : new LangMessages(messages);

    await this.sendToApi(msgCollection, options);

    return msgCollection;
  }

  private async sendToApi(msgCollection: LangMessages, options?: LangOptions): Promise<void> {
    const bodyPart = prepareBodyPartForOpenAIResponsesAPI(msgCollection);

    const body = {
      model: this.model,
      ...{ stream: true },
      ...bodyPart,
      // @TODO: have a function that calculates the maxTokens (take a look at other providers for that)
      ...(typeof options?.maxTokens === 'number' ? { max_output_tokens: options.maxTokens } : {}),
    };

    const req = {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Accept": "text/event-stream",
        Authorization: `Bearer ${this.apiKey}`
      },
      body: JSON.stringify(body),
    };

    const inProgressResponses = new InProgressOpenAIResponse(msgCollection, options?.onResult);
    const response = await fetch(`${this.baseURL}/responses`, req);
    await processResponseStream(response, (data) => inProgressResponses.handleEvent(data));
  }

}